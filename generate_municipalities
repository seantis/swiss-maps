#!/usr/bin/env python3
import json
import os
import subprocess
import sys
from datetime import date
from itertools import groupby
from tempfile import TemporaryDirectory
import fiona


year = sys.argv[1] or date.today().year
try:
    year = int(sys.argv[1])
except IndexError:
    year = date.today().year

fn = 'swissBOUNDARIES3D_1_3_LV03_LN02.gdb'
root = os.getcwd()
gdb_path = f'{root}/src/swissBOUNDARIES3D/{year}/{fn}'

layers = fiona.listlayers(gdb_path)

CANTONS = 'Kantone'
DISTRICTS = 'Bezirke'
MUNICIPALITIES = 'Gemeinden'


# Layer attribute names
BFS_NR = 'BFS_NUMMER'
KTNR = 'KANTONSNUMMER'
UPDATED = 'REVISION_JAHR'
BEZNR = 'BEZIRKSNUMMER'

CANTONS_ABBREV = {
    1: 'ZH',
    2: 'BE',
    3: 'LU',
    4: 'UR',
    5: 'SZ',
    6: 'OW',
    7: 'NW',
    8: 'GL',
    9: 'ZG',
    10: 'FR',
    11: 'SO',
    12: 'BS',
    13: 'BL',
    14: 'SH',
    15: 'AR',
    16: 'AI',
    17: 'SG',
    18: 'GR',
    19: 'AG',
    20: 'TG',
    21: 'TI',
    22: 'VD',
    23: 'VS',
    24: 'NE',
    25: 'GE',
    26: 'JU'
}


def prop(entry, key):
    return entry['properties'][key]


def year_warning(entry):
    updated = prop(entry, UPDATED)
    if updated < year:
        name = prop(entry, 'NAME')
        ID = prop(entry, "OBJECT_ID")
        print(f'{name} with ID {ID} is from year {updated}')
    assert not updated > year, f"You are using a dataset newer than {year}"


def convert_to_gpkg(src, dest, show_info=False):
    assert os.path.exists(src)

    subprocess.run([
        'ogr2ogr', '-f', 'GPKG', dest, '-sql',
        "select KANTONSNUMMER, NAME from TLM_KANTONSGEBIET where ICC = \'CH\' "
        "order by KANTONSNUMMER",
        '-nln', CANTONS, src
    ], stdout=subprocess.DEVNULL)

    # Append to the same db
    subprocess.run([
        'ogr2ogr', '-f', 'GPKG', dest, '-update', '-sql',
        "select * from TLM_BEZIRKSGEBIET where ICC = \'CH\'"
        "order by KANTONSNUMMER, BEZIRKSNUMMER",
        '-nln', DISTRICTS, src
    ], stdout=subprocess.DEVNULL)

    subprocess.run([
        'ogr2ogr', '-f', 'GPKG', dest, '-update', '-sql',
        "select * from TLM_HOHEITSGEBIET where ICC = \'CH\' "
        "order by KANTONSNUMMER, BEZIRKSNUMMER, BFS_NUMMER",
        '-nln', MUNICIPALITIES, src
    ], stdout=subprocess.DEVNULL)

    if show_info:
        subprocess.run(['ogrinfo', dest])


cantons = {}
districts = {}
municipalities = {}

with TemporaryDirectory() as tmpdir:
    outfile = f'{tmpdir}/out.gpkg'
    convert_to_gpkg(src=gdb_path, dest=outfile, show_info=True)
    skip_when_missing_district = True

    with fiona.open(outfile, layer=DISTRICTS) as src:
        for entry in src:
            bez_nr = prop(entry, BEZNR)
            districts[bez_nr] = prop(entry, 'NAME')

    def groupkey(entry):
        return entry['properties'][KTNR]

    with fiona.open(outfile, layer=MUNICIPALITIES) as src:
        for nr, entries in groupby(src, groupkey):
            for entry in entries:
                bez_nr = prop(entry, BEZNR)
                if not bez_nr:
                    continue
                year_warning(entry)
                by_canton = municipalities.setdefault(
                    CANTONS_ABBREV[prop(entry, KTNR)].lower(), {}
                )
                by_canton[prop(entry, BFS_NR)] = {
                    'name': prop(entry, 'NAME'),
                    'district': districts[bez_nr]
                }

# write json files
json_dir = f'{root}/municipalities'
all_json = f'{json_dir}/_all.json'


def dump(data):
    return json.dumps(data, sort_keys=True, indent=4)


for kt_abbrev, group in municipalities.items():
    file = f'{json_dir}/{kt_abbrev}.json'
    with open(file, 'w') as f:
        f.write(dump(group))

with open(all_json, 'w') as f:
    f.write(dump(municipalities))









